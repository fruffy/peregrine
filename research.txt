Research Outline


The research project set out to build a virtual assistant for computer networks class. We already know AI models can do a lot out of the box, but lacks expertise in domain-specific tasks. 

Therefore, the first attempt was trying to fine-tune a LLM model, namely Dolly 2-7B. However, we realized it was very time-consuming to set up the model and fine-tune it, especially since we lack a high-quality, large corpus to train it.

Then we attempted to use external memory like a vector database that could be served as long-term memory for the LLM model. So instead of fine-tuning/only fine-tune the model, the result will be used to perform a vector search in the database as a mean of fact check and combating hallucinations.

However, before that was complete, the project pivoted to prompt-engineering. The vision was impacted by the paper on Tree of thought/chain of thought prompting. The overall direction shifted to using prompt engineering instead of fine-tuning to better GPT's responses to computer network tasks.

To start with, I ran the experiment of testing GPT with tcpdump commands on both code generation and code understanding. GPT models (GPT 3.5 turbo and GPT 4) both showed good results.

To further escalate the difficulty level, we tried to design a domain-specific language(DSL) written in BNF notation and test GPT's ability to learn a new grammar and generate expressions for different tasks.

During the experiment, we realized it is very hard to design a difficult DSL, but we learned about GPT prompts with few-shot learning examples are impactful of the quality of generated response.

Then we shifted back to computer network tasks, namely the eBPF programs. We tested whether GPT are able to generate bytecode for given BPF programs written in C. But it seems using GPT as compiler is a ill-conceived notion. Instead, we tried to prompt GPT to generate eBPF programs written in C using natural language and few-shot learning.